{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from fastai.imports import *\nimport warnings, logging\n\nwarnings.simplefilter('ignore')\nlogging.disable(logging.WARNING)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explorative Data Analysis","metadata":{}},{"cell_type":"code","source":"path = Path('us-patent-phrase-to-phrase-matching')\npath.ls()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(path / 'train.csv')\ndf","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe(include='object')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum().sum()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def value_counts(data, columns):\n    for c in columns:\n        print(f'\\n --- {c} --- \\n')\n        v = data[c].value_counts()\n        print(f'{v}')\n        print(f'{c}.max = {v.max()}, {c}.mean = {v.mean()}, {c}.min = {v.min()}')\nvalue_counts(df, ['context', 'anchor', 'target', 'score'])\n","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.target.str.count(' ').max(), df.anchor.str.count(' ').max()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.anchor.value_counts().mean(), df.anchor.value_counts().max()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['section'] = df.context.str[0]","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"value_counts(df, ['section', 'context', 'anchor', 'score'])","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 数据量与分布小结\n- 原始数据 36473 rows × 5 columns，没有空值，没有异常值。\n- 其中包含有 2 种分组类型：context, anchor, 和一个数值结果： score\n- context 分组数据行数 max = 2186， mean = 344, min = 18     部分数据不足\n- anchor 分组数据行数 max = 152, mean = 49, min = 1，        部分数据不足\n- score  分组数据行数 max = 12300, mean = 7294, min = 1154， 数据充足\n\n增强 context 分级特征\n- section 分组数据行数 max = 8019, mean = 4559, min = 1279, 数据充足","metadata":{"tags":[]}},{"cell_type":"markdown","source":"## 字符型数据状况","metadata":{}},{"cell_type":"code","source":"def has_uppercase(data, columns):\n    for c in columns:\n        up = any([ch.isupper() for ch in data[c]])\n        print(f'{c} {\"has\" if up else \"has no\"} upper case')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"has_uppercase(df, ['context', 'anchor', 'target'])","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"只有 context 有 upper case  情况，整体数据都比较干净","metadata":{}},{"cell_type":"markdown","source":"# 设计验收数据集","metadata":{}},{"cell_type":"code","source":"def split_train_valid(data, column):\n    u_data = list(data[column].unique())\n    np.random.seed(42)\n    np.random.shuffle(u_data)\n    \n    val_prop = 0.25\n    val_size = int(len(u_data) * val_prop)\n    val_data = u_data[:val_size]\n    is_val = data.anchor.isin(val_data)\n    \n    idxs = np.arange(len(is_val))\n    trn_idxs = idxs[~is_val]\n    val_idxs = idxs[is_val]\n    return trn_idxs, val_idxs","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn_idxs, val_idxs = split_train_valid(df, 'anchor')\nlen(trn_idxs), len(val_idxs), trn_idxs, val_idxs","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import DatasetDict\ndef get_dataset_dict(dataset, trn_idxs, val_idxs):\n    return DatasetDict({\n      \"train\" : dataset.select(trn_idxs),\n      \"test\" : dataset.select(val_idxs)\n    })","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 方案一 数据预处理","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\nmodel_nm = 'microsoft/deberta-v3-small'\ntokenizer = AutoTokenizer.from_pretrained(model_nm)\ntokenizer.all_special_tokens","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize(dataset, projection):\n    return dataset.map(lambda x: tokenizer(x[projection]))","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mk_inputs(data_frame, sep, columns):\n    return data_frame[columns].apply(lambda x: sep.join(x), axis = 1)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_columns =  ['context', 'target', 'anchor']\nsep = f' {tokenizer.sep_token} '\ndf['inputs'] = mk_inputs(df, sep, input_columns)\ndf.head()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\ndataset = Dataset.from_pandas(df).map(lambda row: tokenizer(row['inputs']), batched=True, remove_columns= input_columns + ['id', 'section', 'inputs'])\ndataset = dataset.rename_column('score', 'label')\ndataset[0]","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dds = get_dataset_dict(dataset,trn_idxs, val_idxs)\ndds","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 准备 transformer arguments, model and trainer","metadata":{}},{"cell_type":"code","source":"def corr(eval_preds): return { \"Pearson\": np.corrcoef(*eval_preds)[0][1] }","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels = 1)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bs = 128\nlr = 8e-5\nepochs = 4\nargs = TrainingArguments(output_dir='outputs', evaluation_strategy='epoch', \n                         weight_decay=0.01, warmup_ratio=0.1,\n                         per_device_train_batch_size= bs, per_device_eval_batch_size= bs * 2,\n                         num_train_epochs=epochs, lr_scheduler_type='cosine', fp16=True,\n                         report_to='none')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(model = model, args=args, tokenizer = tokenizer, train_dataset= dds['train'], eval_dataset=dds['test'], compute_metrics=corr)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 方案二 数据预处理 —— context => [context]","metadata":{}},{"cell_type":"code","source":"df2 = pd.read_csv(path / 'train.csv')\ndf2['section'] = df2.context.str[0]\ndf2['context_2'] = '[' + df2.context + ']'","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_columns =  ['context_2', 'target', 'anchor']\nsep = f' {tokenizer.sep_token} '\ndf2['inputs'] = mk_inputs(df2, sep, input_columns)\ndf2.head()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset2 = Dataset.from_pandas(df2).map(lambda row: tokenizer(row['inputs']), batched=True, remove_columns= input_columns + ['id', 'section', 'inputs', 'context'])\ndataset2 = dataset2.rename_column('score', 'label')\ndataset2","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dds2 = get_dataset_dict(dataset2,trn_idxs, val_idxs)\ndds2","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer2 = Trainer(model = model, args=args, tokenizer = tokenizer, train_dataset= dds2['train'], eval_dataset=dds2['test'], compute_metrics=corr)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer2.train()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 方案三 数据预处理 —— 自定义分隔符 [SEP] => [S]","metadata":{}},{"cell_type":"code","source":"df3 = pd.read_csv(path / 'train.csv')\ndf3['section'] = df3.context.str[0]\ndf3['context_2'] = '[' + df3.context + ']'\n\ninput_columns =  ['context_2', 'target', 'anchor']\nsep = f' [S] '\ndf3['inputs'] = mk_inputs(df3, sep, input_columns)\ndf3.head()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset3 = Dataset.from_pandas(df3).map(lambda row: tokenizer(row['inputs']), batched=True, remove_columns= input_columns + ['id', 'section', 'inputs', 'context'])\ndataset3 = dataset3.rename_column('score', 'label')\ndataset3","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dds3 = get_dataset_dict(dataset3,trn_idxs, val_idxs)\ndds3","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer3 = Trainer(model = model, args=args, tokenizer = tokenizer, train_dataset= dds3['train'], eval_dataset=dds3['test'], compute_metrics=corr)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer3.train()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 方案四 数据预处理 —— inputs 小写","metadata":{}},{"cell_type":"code","source":"df4 = pd.read_csv(path / 'train.csv')\ndf4['section'] = df4.context.str[0]\ndf4['context_2'] = '[' + df4.context + ']'\n\ninput_columns =  ['context_2', 'target', 'anchor']\nsep = f' [S] '\ndf4['inputs'] = mk_inputs(df4, sep, input_columns).str.lower()\ndf4.head()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset4 = Dataset.from_pandas(df4).map(lambda row: tokenizer(row['inputs']), batched=True, remove_columns= input_columns + ['id', 'section', 'inputs', 'context'])\ndataset4 = dataset4.rename_column('score', 'label')\ndds4 = get_dataset_dict(dataset4,trn_idxs, val_idxs)\ntrainer4 = Trainer(model = model, args=args, tokenizer = tokenizer, train_dataset= dds4['train'], eval_dataset=dds4['test'], compute_metrics=corr)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer4.train()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 方案五 数据预处理 —— 增加一级分类特殊符号","metadata":{}},{"cell_type":"code","source":"df5 = pd.read_csv(path / 'train.csv')\ndf5['section'] = df5.context.str[0]\ndf5['sectok'] = '[' + df5.section + ']'\nspecial_tokens = list(df5['sectok'].unique())\nspecial_tokens","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.add_special_tokens({\"additional_special_tokens\" : special_tokens})\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df5['context_2'] = '[' + df5.context + ']'\n\ninput_columns =  ['sectok', 'context_2', 'target', 'anchor']\nsep = f' [S] '\ndf5['inputs'] = mk_inputs(df5, sep, input_columns).str.lower()\ndf5.head()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset5 = Dataset.from_pandas(df5).map(lambda row: tokenizer(row['inputs']), batched=True, remove_columns= input_columns + ['id', 'section', 'inputs', 'context'])\ndataset5 = dataset5.rename_column('score', 'label')\ndds5 = get_dataset_dict(dataset5,trn_idxs, val_idxs)\ntrainer5 = Trainer(model = model, args=args, tokenizer = tokenizer, train_dataset= dds5['train'], eval_dataset=dds5['test'], compute_metrics=corr)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer5.train()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 方案六 分层分组","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedGroupKFold","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_folds = 4\ncv = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=42)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idxs = df5.index\nscores = 100 * df5.score\nfolds = list(cv.split(idxs, scores, df5.anchor))\nfolds","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 1\nfor trn_idxs, val_idxs in folds:\n    print(f'fold {i}')\n    dds = get_dataset_dict(dataset5,trn_idxs, val_idxs)\n    trainer = Trainer(model = model, args=args, tokenizer = tokenizer, train_dataset= dds['train'], eval_dataset=dds['test'], compute_metrics=corr)\n    trainer.train()\n    metrics = [o['eval_Pearson'] for o in trainer.state.log_history if 'eval_Pearson' in o]\n    metrics[-1]\n    i += 1","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}